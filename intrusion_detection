
# Import Libraries and Packages
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn for preprocessing, model selection, and classification
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split

# Machine Learning Models
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Metrics for Model Evaluation
from sklearn.metrics import accuracy_score, confusion_matrix

# Set Seaborn style for better visualization
sns.set_style("whitegrid")


# Set the directory containing dataset files
dataset_path = r"E:\Aish's Office\Mtech\assignments\Cyber\KDD_Datafiles"

# Check if all required files exist
print("Files in Dataset Folder:")
print(os.listdir(dataset_path))

# Read and Display kddcup.names (Contains Feature Names)
names_file = os.path.join(dataset_path, "kddcup.names")
if os.path.exists(names_file):
    with open(names_file, "r") as f:
        print("\nkddcup.names File Content:")
        print(f.read())
else:
    print("\nkddcup.names file not found!")

# Read and Display training_attack_types (Contains Attack Categories)
attack_types_file = os.path.join(dataset_path, "training_attack_types.txt")
if os.path.exists(attack_types_file):
    with open(attack_types_file, "r") as f:
        print("\ntraining_attack_types File Content:")
        print(f.read())
else:
    print("\ntraining_attack_types file not found!")

# Define Dataset File
dataset_file = os.path.join(dataset_path, "kddcup.data_10_percent.gz")

# Define Column Names Manually (From kddcup.names)
columns = [
    "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes", "land", "wrong_fragment", "urgent",
    "hot", "num_failed_logins", "logged_in", "num_compromised", "root_shell", "su_attempted", "num_root",
    "num_file_creations", "num_shells", "num_access_files", "num_outbound_cmds", "is_host_login", "is_guest_login",
    "count", "srv_count", "serror_rate", "srv_serror_rate", "rerror_rate", "srv_rerror_rate", "same_srv_rate",
    "diff_srv_rate", "srv_diff_host_rate", "dst_host_count", "dst_host_srv_count", "dst_host_same_srv_rate",
    "dst_host_diff_srv_rate", "dst_host_same_src_port_rate", "dst_host_srv_diff_host_rate", "dst_host_serror_rate",
    "dst_host_srv_serror_rate", "dst_host_rerror_rate", "dst_host_srv_rerror_rate", "target"
]

# Read the Dataset
if os.path.exists(dataset_file):
    df = pd.read_csv(dataset_file, compression="gzip", names=columns)
    print("\nDataset Loaded Successfully!")
    print(f"Dataset Shape: {df.shape}")
    print(df.head())
else:
    print("\nError: Dataset file not found!")


# Drop the 'service' column
df.drop(columns=['service', 'count', 'diff_srv_rate', 'same_srv_rate', 'src_bytes','serror_rate', 'dst_bytes'], inplace=True)

# Map categorical features to numerical values
pmap = {'icmp': 0, 'tcp': 1, 'udp': 2}
df['protocol_type'] = df['protocol_type'].map(pmap)

fmap = {'SF': 0, 'S0': 1, 'REJ': 2, 'RSTR': 3, 'RSTO': 4, 'SH': 5,
        'S1': 6, 'S2': 7, 'RSTOS0': 8, 'S3': 9, 'OTH': 10}
df['flag'] = df['flag'].map(fmap)

# Verify if mapping was applied correctly
print("\nUnique values in 'protocol_type':", df['protocol_type'].unique())
print("Unique values in 'flag':", df['flag'].unique())

print("\nDataset Information BEFORE Label Conversion:")
df.info()

# Convert "target" column to Binary Classification (0 = Normal, 1 = Attack)
df['label'] = df['target'].astype(str).str.strip().apply(lambda x: 0 if x == 'normal.' else 1)
print("Unique Label Values:", df['label'].unique())  # Should only print [0, 1]

# Drop "target" column after conversion
df.drop(columns=['target'], inplace=True)

# Display Basic Dataset Information
print("Dataset Information:")
df.info()
print("Dataset Shape:", df.shape)
print("First Few Rows:")
print(df.head())

# Check for Missing Values and Duplicates
print("\nMissing Values in Dataset:")
print(df.isnull().sum().sum())

print("\nChecking for Duplicate Rows:")
print(f"Total Duplicates: {df.duplicated().sum()}")

df.drop_duplicates(inplace=True)
print(f"Dataset after removing duplicates: {df.shape}")
# Function to Remove Highly Correlated Features
def remove_highly_correlated_features(df, threshold=0.95):
    corr_matrix = df.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
    return to_drop

# Define Label Column
label_column = 'label'

# Remove Highly Correlated Features
correlated_features = remove_highly_correlated_features(df.drop(columns=[label_column]))
print("\nRemoving Highly Correlated Features:", correlated_features)
df.drop(columns=correlated_features, inplace=True, errors='ignore')

# Apply Variance Threshold for Feature Selection
selector = VarianceThreshold(threshold=0.01)  # Removing features with very low variance
X = df.drop(columns=[label_column])
X_selected = selector.fit_transform(X)

# Create DataFrame with Selected Features
df_selected = pd.DataFrame(X_selected, columns=X.columns[selector.get_support()])

# Retain Label Column
df_selected[label_column] = df[label_column].values
df = df_selected  # Replace original DataFrame

# Standardize Numeric Features
scaler = StandardScaler()
numeric_columns = df.drop(columns=[label_column]).select_dtypes(include=[np.number]).columns
df[numeric_columns] = scaler.fit_transform(df[numeric_columns])

# Log Dropped Features for Debugging
dropped_features_log = {
    "correlated_features": correlated_features,
    "low_variance_features": list(set(X.columns) - set(df_selected.columns))
}

print("\nDropped Features Log:", dropped_features_log)

# Display Correlation Heatmap After Removing Highly Correlated Features
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
plt.title("Feature Correlation Heatmap After Removal")
plt.show()

# Display Categorical Feature Distributions
categorical_columns = ['protocol_type', 'flag']
for col in categorical_columns:
    if col in df.columns:
        plt.figure(figsize=(4, 2))
        sns.countplot(x=df[col])
        plt.title(f"Distribution of {col}")
        plt.xticks(rotation=90)
        plt.show()

# Display Numerical Feature Distributions
df.hist(figsize=(15, 10), bins=50)
plt.suptitle("Numerical Feature Distributions")
plt.show()

# Display Target Label Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x=df['label'], palette='coolwarm')
plt.title("Target Label Distribution")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

# Display Final Processed Data
print("\nFeature Selection and Preprocessing Completed!")
print(f"Dataset Shape After Feature Selection: {df.shape}")
print(df.head(3))

# Define features and target
X = df.drop(columns=['label'])
y = df['label']

# Feature Selection Completed - Now Apply RFE
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# Define the model for feature selection
rfe_model = RandomForestClassifier(n_estimators=10)

# Apply RFE to retain only the top 15 features
rfe = RFE(rfe_model, n_features_to_select=15)
X_reduced = rfe.fit_transform(X, y)

# Get the selected feature names
selected_features = X.columns[rfe.support_]
print("Selected Features After RFE:", list(selected_features))

# Convert X_reduced back to a DataFrame with selected feature names
X = pd.DataFrame(X_reduced, columns=selected_features)

# Split the dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y
)

# Print shapes to confirm
print("Training Set Shape:", X_train.shape, y_train.shape)
print("Testing Set Shape:", X_test.shape, y_test.shape)



from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score

# Function to train and evaluate models
def train_and_evaluate(model, model_name):
    print(f"\nTraining {model_name}...")
    
    # Train model
    model.fit(X_train, y_train)
    
    # Predictions on Training and Test Set
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Compute Metrics
    train_accuracy = accuracy_score(y_train, y_train_pred) * 100
    test_accuracy = accuracy_score(y_test, y_test_pred) * 100
    
    test_precision = precision_score(y_test, y_test_pred) * 100
    test_recall = recall_score(y_test, y_test_pred) * 100
    test_f1 = f1_score(y_test, y_test_pred) * 100

    # Estimate Loss as (1 - Accuracy)
    train_loss = 100 - train_accuracy
    test_loss = 100 - test_accuracy
    
    print(f"\n{model_name} Performance:")
    print(f"Training Accuracy: {train_accuracy:.2f}%, Training Loss: {train_loss:.2f}%")
    print(f"Test Accuracy: {test_accuracy:.2f}%, Test Loss: {test_loss:.2f}%")
    print(f"Test Precision: {test_precision:.2f}%, Test Recall: {test_recall:.2f}%, Test F1-Score: {test_f1:.2f}%")
    
    # Confusion Matrix
    plt.figure(figsize=(5,4))
    sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted Label")
    plt.ylabel("Actual Label")
    plt.show()
    
    return train_accuracy, train_loss, test_accuracy, test_loss, test_precision, test_recall, test_f1


# Calling the function for different models
nb_train_acc, nb_train_loss, nb_test_acc, nb_test_loss, nb_test_precision, nb_test_recall, nb_test_f1 = train_and_evaluate(GaussianNB(), "Na√Øve Bayes")
dt_train_acc, dt_train_loss, dt_test_acc, dt_test_loss,  dt_test_precision, dt_test_recall, dt_test_f1= train_and_evaluate(DecisionTreeClassifier(max_depth=10, min_samples_split=5,min_samples_leaf=3), "Decision Tree")
svm_train_acc, svm_train_loss, svm_test_acc, svm_test_loss,  svm_test_precision, svm_test_recall, svm_test_f1= train_and_evaluate(SVC(kernel='poly', degree=3, C=1), "Support Vector Machine")
rf_train_acc, rf_train_loss, rf_test_acc, rf_test_loss, rf_test_precision, rf_test_recall, rf_test_f1= train_and_evaluate(RandomForestClassifier(n_estimators=50, max_depth=5, min_samples_split=30), "Random Forest")
